{
  "name": "llama-swap",
  "version": "1.0.0",
  "description": "A reverse proxy for local LLM models with dynamic switching",
  "main": "server.js",
  "scripts": {
    "start": "node server.js",
    "dev": "nodemon server.js",
    "test": "jest"
  },
  "keywords": [
    "llm",
    "reverse-proxy",
    "llama",
    "openai",
    "models"
  ],
  "author": "Llama Swap Team",
  "license": "PRIVATE",
  "dependencies": {
    "express": "^4.18.2",
    "yaml": "^2.3.4",
    "http-proxy-middleware": "^2.0.6",
    "cors": "^2.8.5",
    "express-rate-limit": "^7.1.5",
    "winston": "^3.11.0",
    "commander": "^11.1.0"
  },
  "devDependencies": {
    "nodemon": "^3.0.2",
    "jest": "^29.7.0",
    "supertest": "^6.3.3"
  }
}